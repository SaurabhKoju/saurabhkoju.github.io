<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Flow Matching | Saurabh Koju </title> <meta name="author" content="Saurabh Koju"> <meta name="description" content="An introduction to Flow Matching"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://saurabhkoju.github.io/blog/2025/flows/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Saurabh</span> Koju </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Flow Matching</h1> <p class="post-meta"> Created on June 27, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/flows"> <i class="fa-solid fa-hashtag fa-sm"></i> flows</a>   <a href="/blog/tag/generative-modelling"> <i class="fa-solid fa-hashtag fa-sm"></i> generative-modelling</a>   ·   <a href="/blog/category/generative-modelling"> <i class="fa-solid fa-tag fa-sm"></i> generative-modelling</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>This write-up is primarily based on the excellent <a href="https://diffusion.csail.mit.edu/docs/lecture-notes.pdf" rel="external nofollow noopener" target="_blank">lecture-notes</a> , which I encourage the readers to check out. I have tried to condense the content and simplify some derivations. I expect to follow up with a separate write-up on Diffusion and SDE.</p> <h4 id="setup">Setup</h4> <p>We want to model some data generating distribution $p_{data}$ using samples $(y_1, y_2, …, y_n)$ drawn from this distribution. We will do so by defining a map from samples from some simple distribution $p_{init}$ (for example, a gaussian distribution with zero mean and unit covariance matrix) to samples from our data generating distribution $p_{data}$. Towards this, we will define a family of intermediate variables $x_t$ drawn from distributions $p_t$ where $p_0 = p_{init}$ and $p_1 = p_{data}$ for $t \in [0, 1]$. Note that $x_t$ is defined to have the same dimensionality as the data distribution. The trajectory of $x_t$ is defined using the $ODE$:</p> \[\frac{d}{dt}x_t = u_t(x_t)\] <p>where we want to learn $u_t$ such that drawing different samples $x_0$ from $p_{init}$ and following this $ODE$ from t = 0 to t = 1 will lead to different samples from $p_{data}$. <br> Note that $u_t$ is time dependent and learns a function with same input/output dimensionality as the data. Thus we can interpret it as learning a vector field in the data space which tells at each time how to nudge each point such that starting from points drawn from $p_{init}$ at t = 0 and evolving it to t = 1, we end up with different samples from $p_{data}$. <br> Here $u_t$ is said to describe a flow.</p> <h4 id="sampling">Sampling</h4> <p>If we have such a $u_t$ we can simulate it to draw samples from $p_{data}$, for example, using the Euler Method with:</p> \[\begin{aligned} x_0 &amp;\sim p_{init}&amp; \\ x_{t+h} &amp;= x_t + h \cdot u_t(x_t) \quad\ \ t = [0, h, 2h, ..., (n-1)h] \end{aligned}\] <p>where $n$ is number of steps and $h=1/n$ is the step-size. <br> To learn such a $u_t$ we will try to explicitly construct a target $u_t$ which has this desired property.</p> <h4 id="conditional-flow">Conditional Flow</h4> <p>First, instead of constructing flow for the whole data, we will construct flow for any single data-point $y$: $u_{t, y}$ such that evolving this flow from t = 0 to t = 1 will land us on $y$ for any $x_0$ drawn from $p_{init}$.</p> \[\begin{aligned} x_0 &amp;\sim p_0 \\ \frac{d}{dt}x_t \quad&amp;= u_{t,y}(x_t) \\ x_1 \quad&amp;= y \end{aligned}\] <p>To construct this flow we can construct a set of <em>conditional interpolating distributions</em> $p(x_t \mid y)$ which describes the distribution of $x_t$ at each t. We have $p(x_0 \mid y)$ = $p_{init}$ and since $p(x_t \mid y)$ is concentrated on a single point y at t = 1, we can model this as a Dirac-Delta distribution $\delta(y)$. It is called <em>conditional interpolating distribution</em> because for a given y it interpolates between $p_{init}$ and the sample y. We can use this later to construct a <em>marginal interpolating distribution</em> and the corresponding <em>marginal flow</em> which will interpolate between $p_{init}$ and $p_{data}$ as required.</p> <h5 id="example">Example</h5> <p>We can define an example <em>conditional interpolating distribution</em> using gaussian distribution as:</p> \[p(x_t|y) = \mathcal{N}(\alpha_t\ y, \beta_t\ I)\] <p>where $\alpha_t$ and $\beta_t$ are defined such that</p> \[\begin{align*} \alpha_0 = 0 \quad \beta_0 = 1 \\ \alpha_1 = 1 \quad \beta_1 = 0 \end{align*}\] <p>Verify for yourself that $p_{init} := p(x_0 \mid y) = \mathcal{N}(0, I)$ and $p(x_1 \mid y) = \delta(y)$. <br> We can construct a <em>conditional trajectory</em> $x_t$ that each $x_0 \sim p_{init}$ takes such that $x_t$ will follow the <em>conditional interpolating distribution</em> $p(x_t \mid y)$ by defining it as:</p> \[x_t = \alpha_ty + \beta_tx_0\] <p>for each $x_0$ <br> Since $x_0$ itself is normally distributed with zero mean and unit co-variance, you can verify for yourself that the defined $x_t$ will follow the required Normal distribution outlined above. <br> We simply have to take time derivative of this $x_t$ to get the <em>conditional flow</em></p> \[u_{t,y}(x_t) = \frac{d}{dt}x_t = \dot{\alpha_t}y+\dot{\beta_t}x_0 = \dot{\alpha_t}y+\frac{\dot{\beta_t}}{\beta_t}(x_t - \alpha_ty))\] <h4 id="marginal-flow">Marginal Flow</h4> <p>Define <em>marginal interpolating distribution</em> of random variable $x_t$ as</p> \[p(x_t) = \int p(x_t|y)p_{data}(y) \, dy\] <p>i.e. to sample $x_t$, we sample $y$ from $p_{data}$, then we sample $x_t$ from $p(x_t \mid y)$. The resulting distribution of $x_t$ is defined as the <em>marginal interpolating distribution</em>. <br> We can verify that $p(x_0) = p_{init}$ and $p(x_1) = p_{data}$ using the definition of Dirac-Delta function. Thus if we can construct a flow to follow this distribution, we can sample from the data distribution by simulating the $ODE$ as outlined above.</p> <p><strong>Remarkable Fact #1</strong> It turns out that the <em>marginal flow</em> of this distribution can be defined in terms of <em>conditional flow</em> as:</p> \[u_t(x_t) = \mathbb{E}_{y \sim p(y|x_t)}[u_{t, y}(x_t)]\] <p>where $p(y \mid x_t)$ is the posterior distribution of $y$ given $x_t$ which using the Bayes rule, can be expressed as:</p> \[p(y|x_t) = \frac{p(x_t|y)\cdot p_{data}(y)}{p(x_t)}\] <p>For intuition, if we think about finitely many $y’s$, the <em>marginal flow</em> for $x_t$ can be obtained by averaging the <em>conditional flow</em> needed to reach $y$ over all the (non-unique) $y’s$ that $x_t$ could have come from. Refer to appendix for a proof.</p> <h4 id="training-target">Training Target</h4> <p>For any given $y$ we have seen how we can derive a conditional flow we want our model to approximate. For example, in the gaussian case we derived:</p> \[u_{t,y}^{target}(x_t) = \dot{\alpha_t}y+\frac{\dot{\beta_t}}{\beta_t}(x_t - \alpha_ty)\] <p>And the <em>marginal flow</em> we want to actually learn has the form:</p> \[u_t^{target}(x_t) = \mathbb{E}_{y \sim p(y|x_t)}[u_{t, y}^{target}(x_t)]\] <p>If we could calculate this, we could define the flow matching loss as:</p> \[\begin{aligned} \mathcal{L}(\theta) &amp;= \mathbb{E}_{t \sim [0,1],\ x_t \sim p(x_t)} \left\| u_t^{\text{target}}(x_t) - u_t^\theta(x_t) \right\|^2 \\ &amp;= \mathbb{E}_{t \sim [0,1],\ x_t \sim p(x_t)} \left\| \mathbb{E}_{y \sim p(y|x_t)}[u_{t, y}^{target}(x_t)] - u_t^\theta(x_t) \right\|^2 \end{aligned}\] <p>That is for different timesteps, sample a bunch of $x_t$ and try to match flow for $x_t$ against the target flow which is average of the <em>conditional flow</em> for all the $y$ the $x_t$ could have come from. Since we cannot easily sample from the posterior to estimate the inner expectation, we will apply the following trick to rewrite the loss as:</p> \[\begin{aligned} \mathcal{L}(\theta) &amp;= \mathbb{E}_{t \sim [0,1],\ x_t \sim p(x_t)} \left\| \mathbb{E}_{y \sim p(y|x_t)}[u_{t, y}^{target}(x_t)] - u_t^\theta(x_t) \right\|^2 \\ &amp;= \mathbb{E}_{t \sim [0,1],\ x_t \sim p(x_t)} \left\| \mathbb{E}_{y \sim p(y|x_t)}[u_{t, y}^{target}(x_t)] - \mathbb{E}_{y \sim p(y|x_t)}[u_t^\theta(x_t)] \right\|^2 \end{aligned}\] <p>where, since the second term does not depend on $y$, rewriting it as expectation over $y$ makes no difference. Now the loss becomes,</p> \[\begin{aligned} \mathcal{L}(\theta) &amp;= \mathbb{E}_{t \sim [0,1],\ x_t \sim p(x_t)} \left\| \mathbb{E}_{y \sim p(y|x_t)}[u_{t, y}^{target}(x_t) - u_t^\theta(x_t)] \right\|^2 \\ &amp;= \mathbb{E}_{t \sim [0,1],\ x_t \sim p(x_t)}\left[ \mathbb{E}_{y \sim p(y|x_t)}\left\|u_{t, y}^{target}(x_t) - u_t^\theta(x_t) \right\|^2 - \operatorname{Var}_{y \sim p(y|x_t)}[u_{t, y}^{target}(x_t) - u_t^\theta(x_t)]\right]\\ \end{aligned}\] <p>Here we used the identity $\operatorname{Var}[x] = \mathbb{E}[x^2]-\mathbb{E}[x]^2$ where $\operatorname{Var}[\cdot]$ denotes the element-wise variance of the vector-valued expression, i.e., the variance is computed independently for each coordinate of the vector. Furthermore, since the second term inside the variance: $u_{t}^{\theta}(x_t)$ does not depend on $y$, it works out as:</p> \[\operatorname{Var}_{y \sim p(y|x_t)}[u_{t, y}^{target}(x_t) - u_t^\theta(x_t)] = \operatorname{Var}_{y \sim p(y|x_t)}[u_{t, y}^{target}(x_t)]\] <p>which does not depend on the parameters $\theta$. Thus we can omit this term and write the new loss with the squared term now inside the expectation as:</p> \[\begin{aligned} \mathcal{L}(\theta) &amp;= \mathbb{E}_{t \sim [0,1],\ x_t \sim p(x_t)}\left[ \mathbb{E}_{y \sim p(y|x_t)} \left\| u_{t, y}^{target}(x_t) - u_t^\theta(x_t) \right\|^2 \right]\\ &amp;= \mathbb{E}_{t \sim [0,1],\ x_t \sim p(x_t),\ y \sim p(y|x_t)} \left\| u_{t, y}^{target}(x_t) - u_t^\theta(x_t) \right\|^2 \\ &amp;= \mathbb{E}_{t \sim [0,1],\ y \sim p_{data}(y),\ x_t \sim p(x_t|y)} \left\| u_{t, y}^{target}(x_t) - u_t^\theta(x_t) \right\|^2 \\ \end{aligned}\] <p>where, for the final step, we rewrite the expectation over the joint distribution $p(x_t,y)$ as one over $y∼p(y)$, followed by $x_t∼p(x_t \mid y)$. <br> Notice that in the final form we don’t need to sample from the posterior distribution. Thus we can train our model with minibatch gradient descent. We summarize the training and sampling algorithms in the following section.</p> <h4 id="final-algorithm">Final Algorithm</h4> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Saurabh Koju. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: June 28, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>