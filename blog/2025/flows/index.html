<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Flow Matching | Saurabh Koju </title> <meta name="author" content="Saurabh Koju"> <meta name="description" content="An introduction to Flow Matching"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pseudocode@2.4.1/build/pseudocode.min.css" integrity="sha256-VwMV//xgBPDyRFVSOshhRhzJRDyBmIACniLPpeXNUdc=" crossorigin="anonymous"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://saurabhkoju.github.io/blog/2025/flows/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Flow Matching",
            "description": "An introduction to Flow Matching",
            "published": "June 27, 2025",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Saurabh</span> Koju </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Flow Matching</h1> <p>An introduction to Flow Matching</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#setup">Setup</a> </div> <div> <a href="#sampling">Sampling</a> </div> <div> <a href="#conditional-flow">Conditional Flow</a> </div> <div> <a href="#"></a> </div> <ul> <li> <a href="#example">Example</a> </li> </ul> <div> <a href="#marginal-flow">Marginal Flow</a> </div> <div> <a href="#training-target">Training Target</a> </div> <div> <a href="#final-algorithm">Final Algorithm</a> </div> <div> <a href="#"></a> </div> <ul> <li> <a href="#training-algorithm">Training Algorithm</a> </li> <li> <a href="#sampling-algorithm">Sampling Algorithm</a> </li> </ul> </nav> </d-contents> <p>This write-up is primarily based on the excellent <a href="https://diffusion.csail.mit.edu/docs/lecture-notes.pdf" rel="external nofollow noopener" target="_blank">lecture-notes</a>, which I encourage the readers to check out. I have tried to condense the content and simplify some derivations. I expect to follow up with a separate write-up on Diffusion and SDE.</p> <h2 id="setup">Setup</h2> <p>We want to model some data generating distribution $p_{data}$ using samples $(y_1, y_2, â€¦, y_n)$ drawn from this distribution. We will do so by defining a map from samples from some simple distribution $p_{\text{init}}$ (for example, a gaussian distribution with zero mean and unit covariance matrix) to samples from our data generating distribution $p_{data}$. Towards this, we will define a family of intermediate variables $x_t$ drawn from distributions $p_t$ where $p_0 = p_{\text{init}}$ and $p_1 = p_{data}$ for $t \in [0, 1]$. Note that $x_t$ is defined to have the same dimensionality as the data distribution. The trajectory of $x_t$ is defined using the ODE:</p> \[\frac{d}{dt}x_t = u_t(x_t)\] <p>where we want to learn $u_t$ such that drawing different samples $x_0$ from $p_{\text{init}}$ and following this ODE from $t = 0$ to $t = 1$ will lead to different samples from $p_{data}$. <br> Note that $u_t$ is time dependent and learns a function with same input/output dimensionality as the data. Thus, we can interpret it as learning a vector field in the data space which tells at each time how to nudge each point such that starting from points drawn from $p_{\text{init}}$ at $t = 0$ and evolving it to $t = 1$, we end up with different samples from $p_{data}$. <br> Here $u_t$ is said to describe a flow.</p> <h2 id="sampling">Sampling</h2> <p>If we have such a $u_t$ we can simulate it to draw samples from $p_{data}$, for example, using the Euler Method with:</p> \[\begin{aligned} x_0 &amp;\sim p_{\text{init}}&amp; \\ x_{t+h} &amp;= x_t + h \cdot u_t(x_t) \quad\ \ t = [0, h, 2h, ..., (n-1)h] \end{aligned}\] <p>where $n$ is number of steps and $h=1/n$ is the step-size. <br> To learn such a $u_t$ we will try to explicitly construct a target $u_t$ which has this desired property.</p> <h2 id="conditional-flow">Conditional Flow</h2> <p>First, instead of constructing flow for the whole data, we will construct flow for any single data-point $y$: $u_{t, y}$ such that evolving this flow from $t = 0$ to $t = 1$ will land us on $y$ for any $x_0$ drawn from $p_{\text{init}}$.</p> \[\begin{aligned} x_0 &amp;\sim p_0 \\ \frac{d}{dt}x_t \quad&amp;= u_{t,y}(x_t) \\ x_1 \quad&amp;= y \end{aligned}\] <p>To construct this flow we can construct a set of <em>conditional interpolating distributions</em> $p(x_t \mid y)$ which describes the distribution of $x_t$ at each t. We have $p(x_0 \mid y)$ = $p_{\text{init}}$ and since $p(x_t \mid y)$ is concentrated on a single point y at $t = 1$, we can model this as a Dirac-Delta distribution $\delta(y)$. It is called <em>conditional interpolating distribution</em> because for a given $y$ it interpolates between $p_{\text{init}}$ and the sample $y$. We can use this later to construct a <em>marginal interpolating distribution</em> and the corresponding <em>marginal flow</em> which will interpolate between $p_{\text{init}}$ and $p_{data}$ as required.</p> <h3 id="example">Example</h3> <p>We can define an example <em>conditional interpolating distribution</em> using gaussian distribution as:</p> \[p(x_t|y) = \mathcal{N}(\alpha_t\ y, \beta_t\ I)\] <p>where $\alpha_t$ and $\beta_t$ are defined such that</p> \[\begin{align*} \alpha_0 = 0 \quad \beta_0 = 1 \\ \alpha_1 = 1 \quad \beta_1 = 0 \end{align*}\] <p>Verify for yourself that $p_{\text{init}} := p(x_0 \mid y) = \mathcal{N}(0, I)$ and $p(x_1 \mid y) = \delta(y)$. <br> We can now construct a <em>conditional trajectory</em> $x_t$ that each $x_0 \sim p_{\text{init}}$ takes such that $x_t$ will follow the <em>conditional interpolating distribution</em> $p(x_t \mid y)$ by defining the trajectory as:</p> \[x_t = \alpha_ty + \beta_tx_0\] <p>for each $x_0$ <br> Since $x_0$ itself is normally distributed with zero mean and unit co-variance, you can verify for yourself that the defined $x_t$ will follow the required Normal distribution outlined above. <br> We simply have to take time derivative of this $x_t$ to get the <em>conditional flow</em></p> \[u_{t,y}(x_t) = \frac{d}{dt}x_t = \dot{\alpha_t}y+\dot{\beta_t}x_0 = \dot{\alpha_t}y+\frac{\dot{\beta_t}}{\beta_t}(x_t - \alpha_ty))\] <h2 id="marginal-flow">Marginal Flow</h2> <p>Define <em>marginal interpolating distribution</em> of random variable $x_t$ as</p> \[p(x_t) = \int p(x_t|y)p_{data}(y) \, dy\] <p>i.e. to sample $x_t$, we sample $y$ from $p_{data}$, then we sample $x_t$ from $p(x_t \mid y)$. The resulting distribution of $x_t$ is defined as the <em>marginal interpolating distribution</em>. <br> We can verify that $p(x_0) = p_{\text{init}}$ and $p(x_1) = p_{data}$ using the definition of Dirac-Delta function. Thus, if we can construct a flow to follow this distribution, we can sample from the data distribution by simulating the ODE as outlined in the section <a href="#sampling">Sampling</a>.</p> <p><strong>Important Identity</strong> <br> It turns out that the <em>marginal flow</em> of this distribution can be defined in terms of <em>conditional flow</em> as:</p> \[u_t(x_t) = \mathbb{E}_{y \sim p(y|x_t)}[u_{t, y}(x_t)]\] <p>where $p(y \mid x_t)$ is the posterior distribution of $y$ given $x_t$, which using the Bayes rule, can be expressed as:</p> \[p(y|x_t) = \frac{p(x_t|y)\cdot p_{data}(y)}{p(x_t)}\] <p>For intuition, if we think about finitely many $yâ€™s$, the <em>marginal flow</em> for $x_t$ can be obtained by averaging the <em>conditional flow</em> needed to reach $y$ over all the $yâ€™s$ that $x_t$ could have come from. I hope this is somewhat intuitive, you can find the proof in the notes linked at the beginning.</p> <h2 id="training-target">Training Target</h2> <p>For any given $y$ we have seen how we can derive a conditional flow we want our model to approximate. For example, in the gaussian case we derived:</p> \[u_{t,y}^{target}(x_t) = \dot{\alpha_t}y+\frac{\dot{\beta_t}}{\beta_t}(x_t - \alpha_ty)\] <p>And the <em>marginal flow</em> we want to actually learn has the form:</p> \[u_t^{target}(x_t) = \mathbb{E}_{y \sim p(y|x_t)}[u_{t, y}^{target}(x_t)]\] <p>If we could calculate this, we could define the flow matching loss as:</p> \[\begin{aligned} \mathcal{L}(\theta) &amp;= \mathbb{E}_{t \sim [0,1],\ x_t \sim p(x_t)} \left\| u_t^{\text{target}}(x_t) - u_t^\theta(x_t) \right\|^2 \\ &amp;= \mathbb{E}_{t \sim [0,1],\ x_t \sim p(x_t)} \left\| \mathbb{E}_{y \sim p(y|x_t)}[u_{t, y}^{target}(x_t)] - u_t^\theta(x_t) \right\|^2 \end{aligned}\] <p>That is for each timestep and $x_t$, try to match flow for $x_t$ against the target flow which is average of the <em>conditional flow</em> for all the $y$ the $x_t$ could have come from. Since we cannot easily sample from the posterior to estimate the inner expectation, we will apply the following trick to rewrite the loss as:</p> \[\begin{aligned} \mathcal{L}(\theta) &amp;= \mathbb{E}_{t \sim [0,1],\ x_t \sim p(x_t)} \left\| \mathbb{E}_{y \sim p(y|x_t)}[u_{t, y}^{target}(x_t)] - u_t^\theta(x_t) \right\|^2 \\ &amp;= \mathbb{E}_{t \sim [0,1],\ x_t \sim p(x_t)} \left\| \mathbb{E}_{y \sim p(y|x_t)}[u_{t, y}^{target}(x_t)] - \mathbb{E}_{y \sim p(y|x_t)}[u_t^\theta(x_t)] \right\|^2 \end{aligned}\] <p>where, since the second term does not depend on $y$, rewriting it as expectation over $y$ makes no difference. Now the loss becomes,</p> \[\begin{aligned} \mathcal{L}(\theta) &amp;= \mathbb{E}_{t \sim [0,1],\ x_t \sim p(x_t)} \left\| \mathbb{E}_{y \sim p(y|x_t)}[u_{t, y}^{target}(x_t) - u_t^\theta(x_t)] \right\|^2 \\ &amp;= \mathbb{E}_{t \sim [0,1],\ x_t \sim p(x_t)}\left[ \mathbb{E}_{y \sim p(y|x_t)}\left\|u_{t, y}^{target}(x_t) - u_t^\theta(x_t) \right\|^2 - \operatorname{Var}_{y \sim p(y|x_t)}[u_{t, y}^{target}(x_t) - u_t^\theta(x_t)]\right]\\ \end{aligned}\] <p>Here we used the identity $\operatorname{Var}[x] = \mathbb{E}[x^2]-\mathbb{E}[x]^2$ where $\operatorname{Var}[\cdot]$ denotes the element-wise variance of the vector-valued expression, i.e., the variance is computed independently for each coordinate of the vector. Furthermore, since the second term inside the variance: $u_{t}^{\theta}(x_t)$ does not depend on $y$, it works out as:</p> \[\operatorname{Var}_{y \sim p(y|x_t)}[u_{t, y}^{target}(x_t) - u_t^\theta(x_t)] = \operatorname{Var}_{y \sim p(y|x_t)}[u_{t, y}^{target}(x_t)]\] <p>which does not depend on the parameters $\theta$. Thus, we can omit this term and write the new loss with the squared term now inside the expectation as:</p> \[\begin{aligned} \mathcal{L}(\theta) &amp;= \mathbb{E}_{t \sim [0,1],\ x_t \sim p(x_t)}\left[ \mathbb{E}_{y \sim p(y|x_t)} \left\| u_{t, y}^{target}(x_t) - u_t^\theta(x_t) \right\|^2 \right]\\ &amp;= \mathbb{E}_{t \sim [0,1],\ x_t \sim p(x_t),\ y \sim p(y|x_t)} \left\| u_{t, y}^{target}(x_t) - u_t^\theta(x_t) \right\|^2 \\ &amp;= \mathbb{E}_{t \sim [0,1],\ y \sim p_{data}(y),\ x_t \sim p(x_t|y)} \left\| u_{t, y}^{target}(x_t) - u_t^\theta(x_t) \right\|^2 \\ \end{aligned}\] <p>where, for the final step, we rewrite the expectation over the joint distribution $p(x_t,y)$ as one over $yâˆ¼p(y)$, followed by $x_tâˆ¼p(x_t \mid y)$. <br> Notice that in the final form we donâ€™t need to sample from the posterior distribution. Thus, we can train our model with minibatch gradient descent. We summarize the training and sampling algorithms in the following section.</p> <h2 id="final-algorithm">Final Algorithm</h2> <h3 id="training-algorithm">Training Algorithm</h3> <p><strong>Algorithm</strong>: Flow Matching Training <br> <strong>Given</strong>: Data distribution $p_{data}$, model $u_t^\theta$, and $p(x_t \mid y)$</p> <p>For each mini-batch of data do:<br> Â Â Â Â 1. Sample $t \sim \left[0,1\right]$<br> Â Â Â Â 2. Sample $x_t \sim p(x_t \mid y)$<br> Â Â Â Â 3. Compute loss $\mathcal{L}(\theta) = \lVert u_{t, y}^{target}(x_t) - u_t^\theta(x_t) \rVert^2$<br> Â Â Â Â 4. Update model parameters $\theta$ using gradient descent on loss $\mathcal{L}(\theta)$</p> <h3 id="sampling-algorithm">Sampling Algorithm</h3> <p><strong>Algorithm</strong>: Sampling from Flow Model<br> <strong>Given</strong>: Model $u_t^\theta$, number of steps $n$</p> <p>Â Â 1. Set $t = 0$<br> Â Â 2. Set step-size $h = \frac{1}{n}$<br> Â Â 3. Sample $x_0 \sim p_{\text{init}}$<br> Â Â 4. <strong>for</strong> $i = 1, â€¦, n$ <strong>do</strong><br> Â Â Â Â Â Â a. $x_{t+h} = x_t + h \cdot u_t(x_t)$<br> Â Â Â Â Â Â b. $t = t + h$<br> Â Â 5. <strong>end for</strong><br> Â Â 6. <strong>return</strong> $x_1$</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2025 Saurabh Koju. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: June 28, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>